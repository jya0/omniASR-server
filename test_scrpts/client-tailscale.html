<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>omniASR Streaming Client</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            max-width: 800px;
            margin: 2rem auto;
            padding: 0 1rem;
            line-height: 1.5;
        }

        h1 {
            color: #333;
        }

        #status {
            font-weight: bold;
            margin-bottom: 1rem;
            padding: 0.5rem;
            border-radius: 4px;
        }

        .status-disconnected {
            background: #fee;
            color: #c33;
        }

        .status-connected {
            background: #eef;
            color: #33c;
        }

        .status-recording {
            background: #efe;
            color: #3c3;
        }

        #transcript {
            white-space: pre-wrap;
            margin-top: 1rem;
            border: 1px solid #ccc;
            padding: 1rem;
            min-height: 200px;
            border-radius: 4px;
            background: #f9f9f9;
            font-size: 1.1rem;
            line-height: 1.6;
        }

        .confirmed {
            color: #000;
        }

        .pending {
            color: #888;
            font-style: italic;
        }

        button {
            padding: 0.5rem 1rem;
            font-size: 1rem;
            cursor: pointer;
            margin-right: 0.5rem;
        }

        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
    </style>
</head>

<body>
    <h1>omniASR Streaming Client</h1>
    <div id="status" class="status-disconnected">Status: Disconnected</div>

    <div>
        <button id="btnConnect">Connect</button>
        <button id="btnStart" disabled>Start Microphone</button>
        <button id="btnStop" disabled>Stop</button>
    </div>

    <div id="transcript"></div>

    <div id="debugAudioContainer"
        style="margin-top: 1rem; padding: 1rem; background: #f9f9f9; border-radius: 8px; border: 1px solid #ddd;">
        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 0.5rem;">
            <strong>Debug Audio Recordings</strong>
            <div id="debugPagination" style="display: flex; gap: 0.5rem; align-items: center;">
                <button id="btnPrevPage" disabled>&lt; Prev</button>
                <span id="pageInfo">Page 1 / 1</span>
                <button id="btnNextPage" disabled>Next &gt;</button>
            </div>
        </div>
        <div id="debugAudio"></div>
    </div>

    <style>
        .config-panel {
            margin: 1rem 0;
            padding: 1rem;
            background: #f0f0f0;
            border-radius: 8px;
            border: 1px solid #ddd;
        }

        .config-group {
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid #e0e0e0;
        }

        .config-group:last-child {
            border-bottom: none;
        }

        .config-row {
            display: flex;
            align-items: center;
            gap: 1rem;
            margin-bottom: 0.5rem;
            flex-wrap: wrap;
        }

        .config-row label {
            font-weight: 500;
            display: flex;
            align-items: center;
            gap: 4px;
        }

        input[type="number"] {
            width: 80px;
            padding: 4px;
        }

        textarea.json-preview {
            width: 100%;
            font-family: monospace;
            font-size: 0.9rem;
            background: #2d2d2d;
            color: #eee;
            padding: 0.5rem;
            border-radius: 4px;
            margin-top: 0.5rem;
            height: 150px;
        }

        summary {
            cursor: pointer;
            font-weight: bold;
            margin-bottom: 0.5rem;
        }

        /* Tooltip Styles */
        .tooltip {
            position: relative;
            display: inline-block;
            cursor: help;
            color: #007bff;
            font-weight: bold;
            margin-left: 2px;
        }

        .tooltip:hover::after {
            content: attr(data-text);
            position: absolute;
            left: 50%;
            bottom: 100%;
            transform: translateX(-50%);
            background: #333;
            color: #fff;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.8rem;
            white-space: wrap;
            width: 200px;
            z-index: 10;
            text-align: center;
        }
    </style>

    <details class="config-panel" open>
        <summary>Advanced Configuration</summary>

        <div class="config-group">
            <div class="config-row">
                <label>
                    Language:
                    <select id="cfg_language">
                        <option value="">(Auto-detect / Default)</option>
                        <option value="eng_Latn">English (eng_Latn)</option>
                        <option value="arb_Arab">Arabic - Standard (arb_Arab)</option>
                        <option value="afb_Arab">Arabic - Gulf (afb_Arab)</option>
                    </select>
                    <span class="tooltip"
                        data-text="Target language for transcription. Leave empty to use server default.">ⓘ</span>
                </label>
            </div>
        </div>

        <div class="config-group">
            <div class="config-row">
                <label>
                    <input type="checkbox" id="cfg_vad_enabled"> Enable VAD
                    <span class="tooltip"
                        data-text="Voice Activity Detection: Filters out silence and non-speech audio to reduce load and hallucinations.">ⓘ</span>
                </label>
                </label>
            </div>
            <div class="config-row">
                <label>
                    Silence Duration (s):
                    <input type="number" id="cfg_vad_silence" value="0.5" step="0.1" min="0">
                    <span class="tooltip"
                        data-text="Minimum silence required to consider a speech segment ended.">ⓘ</span>
                </label>
            </div>
        </div>

        <div class="config-group">
            <div class="config-row">
                <label>
                    <input type="checkbox" id="cfg_nr_enabled"> Enable Noise Removal
                    <span class="tooltip" data-text="DeepFilterNet: AI-based background noise suppression.">ⓘ</span>
                </label>
            </div>
            <div class="config-row">
                <label>
                    Attenuation (0.0-1.0):
                    <input type="number" id="cfg_nr_att" value="0.4" step="0.1" min="0" max="1">
                    <span class="tooltip"
                        data-text="Strength of noise removal. 1.0 is max, lower allows some ambiance.">ⓘ</span>
                </label>
            </div>
        </div>

        <div class="config-group">
            <div class="config-row">
                <label>
                    <input type="checkbox" id="cfg_comp_enabled" checked> Enable Compressor
                    <span class="tooltip"
                        data-text="Reduces volume of loud sounds and boosts quiet sounds for consistent levels.">ⓘ</span>
                </label>
                <label>
                    Thresh (dB):
                    <input type="number" id="cfg_comp_thresh" value="-20.0" step="1">
                    <span class="tooltip" data-text="Level above which compression starts.">ⓘ</span>
                </label>
                <label>
                    Ratio:
                    <input type="number" id="cfg_comp_ratio" value="4.0" step="0.1">
                    <span class="tooltip"
                        data-text="Amount of compression (e.g., 4:1 reduces 4dB input excess to 1dB).">ⓘ</span>
                </label>
            </div>
        </div>

        <div class="config-group">
            <div class="config-row">
                <label>
                    <input type="checkbox" id="cfg_gate_enabled" checked> Enable Noise Gate
                    <span class="tooltip"
                        data-text="Runs BEFORE VAD: Mutes all audio below a certain volume to remove hiss/static. (Audio-level filter)">ⓘ</span>
                </label>
                <label>
                    Thresh (dB):
                    <input type="number" id="cfg_gate_thresh" value="-40.0" step="1">
                    <span class="tooltip" data-text="Level below which audio is muted.">ⓘ</span>
                </label>
            </div>
        </div>

        <div class="config-group">
            <div class="config-row">
                <label>
                    <input type="checkbox" id="cfg_hallc_energy" checked> Energy Gating
                    <span class="tooltip"
                        data-text="Runs AFTER VAD: Skips model inference on quiet audio chunks to prevent hallucinations. (Controls what gets sent to the model)">ⓘ</span>
                </label>
                <label>
                    <input type="checkbox" id="cfg_hallc_ngram" checked> N-gram Detection
                    <span class="tooltip"
                        data-text="Prevents repetitive loops by checking for repeated phrases.">ⓘ</span>
                </label>
            </div>
            <div class="config-row">
                <label>
                    Max Buffer Duration (s):
                    <input type="number" id="cfg_buffer_dur" value="8.0" step="0.5">
                    <span class="tooltip"
                        data-text="Forces a transcription flush if buffer exceeds this length.">ⓘ</span>
                </label>
            </div>
        </div>

        <div class="config-group">
            <div class="config-row">
                <label>
                    <input type="checkbox" id="cfg_debug_enabled" checked> Enable Debug Audio
                    <span class="tooltip"
                        data-text="Return debug audio recordings for playback. Disable to reduce bandwidth.">ⓘ</span>
                </label>
                <label>
                    Flush Interval (s):
                    <input type="number" id="cfg_debug_interval" value="10.0" step="1" min="1">
                    <span class="tooltip"
                        data-text="How often to return debug audio chunks (prevents memory buildup).">ⓘ</span>
                </label>
            </div>
        </div>

        <div class="config-group">
            <label>Generated Request Body (Config JSON):</label>
            <textarea id="cfg_json_preview" class="json-preview" readonly></textarea>
        </div>
    </details>

    <script>
        const SAMPLE_RATE = 16000;
        let ws = null;
        let audioContext = null;
        let processor = null;
        let inputStream = null;

        const btnConnect = document.getElementById('btnConnect');
        const btnStart = document.getElementById('btnStart');
        const btnStop = document.getElementById('btnStop');
        const statusDiv = document.getElementById('status');
        const transcriptDiv = document.getElementById('transcript');
        const debugAudioDiv = document.getElementById('debugAudio');

        // Pagination state for debug audio
        const btnPrevPage = document.getElementById('btnPrevPage');
        const btnNextPage = document.getElementById('btnNextPage');
        const pageInfoSpan = document.getElementById('pageInfo');
        let debugAudioItems = []; // Array of {fullUrl, vadUrl} objects
        let currentPage = 0;
        const ITEMS_PER_PAGE = 3;

        // Config elements
        const inputs = [
            'cfg_language',
            'cfg_vad_enabled', 'cfg_vad_silence',
            'cfg_nr_enabled', 'cfg_nr_att',
            'cfg_comp_enabled', 'cfg_comp_thresh', 'cfg_comp_ratio',
            'cfg_gate_enabled', 'cfg_gate_thresh',
            'cfg_hallc_energy', 'cfg_hallc_ngram',
            'cfg_buffer_dur',
            'cfg_debug_enabled', 'cfg_debug_interval'
        ].map(id => document.getElementById(id));

        const jsonPreview = document.getElementById('cfg_json_preview');

        function getConfigObj() {
            return {
                type: "config",
                sample_rate: SAMPLE_RATE,
                language: document.getElementById('cfg_language').value || null,
                vad_enabled: document.getElementById('cfg_vad_enabled').checked,
                vad_silence_duration: parseFloat(document.getElementById('cfg_vad_silence').value),
                noise_removal_enabled: document.getElementById('cfg_nr_enabled').checked,
                noise_removal_attenuation: parseFloat(document.getElementById('cfg_nr_att').value),
                compressor_enabled: document.getElementById('cfg_comp_enabled').checked,
                compressor_threshold_db: parseFloat(document.getElementById('cfg_comp_thresh').value),
                compressor_ratio: parseFloat(document.getElementById('cfg_comp_ratio').value),
                noise_gate_enabled: document.getElementById('cfg_gate_enabled').checked,
                noise_gate_threshold_db: parseFloat(document.getElementById('cfg_gate_thresh').value),
                hallucination_energy_gating: document.getElementById('cfg_hallc_energy').checked,
                hallucination_ngram_enabled: document.getElementById('cfg_hallc_ngram').checked,
                max_buffer_duration: parseFloat(document.getElementById('cfg_buffer_dur').value),
                debug_audio_enabled: document.getElementById('cfg_debug_enabled').checked,
                debug_audio_interval: parseFloat(document.getElementById('cfg_debug_interval').value),
            };
        }

        function updatePreview() {
            jsonPreview.value = JSON.stringify(getConfigObj(), null, 2);
        }

        // Attach listeners
        inputs.forEach(el => {
            el.addEventListener('change', updatePreview);
            el.addEventListener('input', updatePreview);
        });

        // Initial update
        updatePreview();

        let finalTranscript = "";

        function setStatus(text, type) {
            statusDiv.textContent = `Status: ${text}`;
            statusDiv.className = `status-${type}`;
        }

        function appendTranscript(text, confirmed, pending, isFinal) {
            if (isFinal) {
                finalTranscript += text + "\n";
                transcriptDiv.innerHTML = `<span class="confirmed">${finalTranscript}</span>`;
            } else {
                // Display history + confirmed part of current + pending part of current
                // If confirmed/pending are not sent by old servers, fallback to 'text' as pending
                let currentConfirmed = confirmed || "";
                let currentPending = pending || (confirmed ? "" : text);

                transcriptDiv.innerHTML = `<span class="confirmed">${finalTranscript}${currentConfirmed}</span><span class="pending">${currentPending}</span>`;
            }
            // Auto scroll
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }

        function renderDebugAudioPage() {
            debugAudioDiv.innerHTML = '';
            const totalPages = Math.max(1, Math.ceil(debugAudioItems.length / ITEMS_PER_PAGE));
            const start = currentPage * ITEMS_PER_PAGE;
            const end = Math.min(start + ITEMS_PER_PAGE, debugAudioItems.length);

            for (let i = start; i < end; i++) {
                const item = debugAudioItems[i];
                const container = document.createElement('div');
                container.style.marginBottom = '0.5rem';
                container.style.padding = '0.5rem';
                container.style.background = '#fff';
                container.style.borderRadius = '4px';

                let html = `<p><strong>Chunk ${i + 1}:</strong></p>`;
                if (item.fullUrl) {
                    html += `<p>Full: <a href="${item.fullUrl}" target="_blank">Download</a> <audio controls src="${item.fullUrl}" style="max-width: 100%;"></audio></p>`;
                }
                if (item.vadUrl) {
                    html += `<p>VAD: <a href="${item.vadUrl}" target="_blank">Download</a> <audio controls src="${item.vadUrl}" style="max-width: 100%;"></audio></p>`;
                }
                container.innerHTML = html;
                debugAudioDiv.appendChild(container);
            }

            pageInfoSpan.textContent = `Page ${currentPage + 1} / ${totalPages}`;
            btnPrevPage.disabled = currentPage === 0;
            btnNextPage.disabled = currentPage >= totalPages - 1;
        }

        btnPrevPage.onclick = () => {
            if (currentPage > 0) {
                currentPage--;
                renderDebugAudioPage();
            }
        };

        btnNextPage.onclick = () => {
            const totalPages = Math.ceil(debugAudioItems.length / ITEMS_PER_PAGE);
            if (currentPage < totalPages - 1) {
                currentPage++;
                renderDebugAudioPage();
            }
        };

        btnConnect.onclick = async () => {
            const host = window.location.hostname || "localhost";
            ws = new WebSocket(`ws://${host}:8001/v1/audio/transcriptions`);

            ws.onopen = () => {
                setStatus("Connected", "connected");
                btnConnect.disabled = true;
                btnStart.disabled = false;
            };

            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                if (data.type === "ready") {
                    console.log("Server ready:", data);
                } else if (data.text !== undefined) {
                    if (data.is_final) {
                        console.log("DEBUG: Final Result Received:", data);
                        if (data.debug_audio_url_vad) {
                            console.log("DEBUG: Found debug_audio_url_vad:", data.debug_audio_url_vad);
                        } else {
                            console.warn("DEBUG: No debug_audio_url_vad in message!");
                        }
                    }
                    appendTranscript(data.text, data.confirmed_text, data.pending_text, data.is_final);

                    // Debug: log if debug audio is present
                    console.log("DEBUG CLIENT: Received message, debug_audio_url:", data.debug_audio_url ? "PRESENT" : "MISSING", ", debug_audio_url_vad:", data.debug_audio_url_vad ? "PRESENT" : "MISSING");

                    // Add debug audio to pagination array if present
                    if (data.debug_audio_url || data.debug_audio_url_vad) {
                        console.log("DEBUG CLIENT: Adding debug audio to pagination, URL length:", data.debug_audio_url?.length);
                        debugAudioItems.push({
                            fullUrl: data.debug_audio_url || null,
                            vadUrl: data.debug_audio_url_vad || null
                        });
                        // Auto-navigate to last page when new audio arrives
                        currentPage = Math.floor((debugAudioItems.length - 1) / ITEMS_PER_PAGE);
                        renderDebugAudioPage();
                    }
                } else if (data.type === "error") {
                    console.error("Server error:", data);
                    alert("Server Error: " + data.message);
                }
            };

            ws.onclose = () => {
                setStatus("Disconnected", "disconnected");
                btnConnect.disabled = false;
                btnStart.disabled = true;
                btnStop.disabled = true;
                stopRecording();
            };

            ws.onerror = (err) => {
                console.error("WebSocket error:", err);
                setStatus("Error (Check Console)", "disconnected");
            };
        };

        btnStart.onclick = async () => {
            try {
                // Send config to server FIRST (before starting audio)
                const configMsg = getConfigObj();
                console.log("Sending config:", configMsg);
                ws.send(JSON.stringify(configMsg));

                // Initialize Audio Context
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: SAMPLE_RATE });

                // Get Microphone Stream
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                inputStream = stream;

                // Create Source
                const source = audioContext.createMediaStreamSource(stream);

                // Create Processor (ScriptProcessorNode is deprecated but widely supported for simple tasks)
                // Buffer size 4096 gives ~256ms latency at 16k.
                // Larger buffer helps prevents dropped frames if main thread is busy.
                processor = audioContext.createScriptProcessor(4096, 1, 1);

                source.connect(processor);
                processor.connect(audioContext.destination); // Start processing

                processor.onaudioprocess = (e) => {
                    const inputData = e.inputBuffer.getChannelData(0);

                    // Convert Float32 to Int16
                    const buffer = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        // Clamp and scale
                        let s = Math.max(-1, Math.min(1, inputData[i]));
                        buffer[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    }

                    // Send to WS if open
                    if (ws && ws.readyState === WebSocket.OPEN) {
                        ws.send(buffer.buffer);
                    }
                };

                setStatus("Recording...", "recording");
                btnStart.disabled = true;
                btnStop.disabled = false;

            } catch (err) {
                console.error("Mic error:", err);
                alert("Could not access microphone. Ensure you are allowing access.");
            }
        };

        function stopRecording() {
            if (inputStream) {
                inputStream.getTracks().forEach(track => track.stop());
                inputStream = null;
            }
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (ws && ws.readyState === WebSocket.OPEN && btnStop.disabled === false) {
                ws.send(JSON.stringify({ type: "end" }));
            }

            btnStart.disabled = ws && ws.readyState === WebSocket.OPEN ? false : true;
            btnStop.disabled = true;
            if (ws && ws.readyState === WebSocket.OPEN) setStatus("Connected", "connected");
        }

        btnStop.onclick = stopRecording;
    </script>
</body>

</html>